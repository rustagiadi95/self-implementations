{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "## data imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "## transformer related imports\n",
    "from transformers import AutoTokenizer\n",
    "from transformers_fmt.model_blocks.transformer import Transformers\n",
    "\n",
    "## constants\n",
    "from constants import ROOT_DIR, DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token' : '[BOS]',\n",
    "    'eos_token' : '[EOS]'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_location, chunksize=1000, n_chunks = 100) :\n",
    "\n",
    "    data = []\n",
    "\n",
    "\n",
    "    for i, items in enumerate(pd.read_csv(file_location, chunksize=chunksize)) :\n",
    "\n",
    "        data.append(items)\n",
    "        if i == n_chunks - 1 :\n",
    "            break\n",
    "\n",
    "    data = pd.concat(data)\n",
    "    data.index = range(len(data))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(file_location=os.path.join(ROOT_DIR, 'data/en-fr.csv'), n_chunks = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data.index = range(len(train_data))\n",
    "val_data.index = range(len(val_data))\n",
    "test_data.index = range(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset) :\n",
    "\n",
    "    def __init__(self, data) :\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) -> any:\n",
    "        row = self.data.loc[index]\n",
    "        return {'en' : row['en'], 'fr' : row['fr']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, type = 'input') :\n",
    "\n",
    "    ## Append the BOS and EOS token based on wether the batch is the encoder input, decoder input(output shifted left)\n",
    "    ## or the label (output shifted right)\n",
    "    if type == 'input' :\n",
    "        input_token_ids = [\n",
    "            torch.cat(\n",
    "                (\n",
    "                    torch.tensor([tokenizer.bos_token_id]), \n",
    "                    torch.tensor(inp), \n",
    "                    torch.tensor([tokenizer.eos_token_id])\n",
    "                ),\n",
    "            ) for inp in batch['input_ids']\n",
    "        ]\n",
    "\n",
    "    elif type == 'output' :\n",
    "        input_token_ids = [\n",
    "            torch.cat(\n",
    "                (torch.tensor([tokenizer.bos_token_id]), torch.tensor(inp)),\n",
    "            ) for inp in batch['input_ids']\n",
    "        ]\n",
    "\n",
    "    elif type == 'label' :\n",
    "        input_token_ids = [\n",
    "            torch.cat(\n",
    "                (torch.tensor(inp), torch.tensor([tokenizer.eos_token_id])),\n",
    "            ) for inp in batch['input_ids']\n",
    "        ]\n",
    "\n",
    "    ## pad the token to the maxiumum sentence length\n",
    "    input_token_ids = pad_sequence(input_token_ids, batch_first=True, padding_value = tokenizer.pad_token_id)\n",
    "\n",
    "    return input_token_ids\n",
    "\n",
    "# def collate_fn(samples):\n",
    "    \n",
    "#     eng_samples = [items['en'] for items in samples]\n",
    "#     fr_samples = [items['fr'] for items in samples]\n",
    "\n",
    "#     batch = {}\n",
    "\n",
    "#     for language, sample in {'en' : eng_samples, 'fr' : fr_samples}.items() :\n",
    "\n",
    "#         sample = tokenizer.batch_encode_plus(sample)\n",
    "#         batch[language] = preprocess_batch(sample)\n",
    "\n",
    "#     # samples['fr'] = tokenizer.batch_encode_plus(samples['fr'])\n",
    "#     return batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dataset = Data(val_data)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = Data(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "data_loaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader,\n",
    "    'test': test_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformers(\n",
    "    n_layer = 6,\n",
    "    n_heads = 8,\n",
    "    d_model = 512, \n",
    "    d_ff = 2048,\n",
    "    max_seq_len = 128,\n",
    "    vocab_size = tokenizer.vocab_size\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, enc_output, tokenizer, max_seq_len):\n",
    "    \n",
    "    input_ids = torch.tensor([[tokenizer.bos_token_id] for _ in range(enc_output.size(0))]).to(device)\n",
    "\n",
    "    unfinished_sequences = torch.ones(input_ids.size(0), 1).to(device)\n",
    "    eos_token_id_tensor = torch.tensor([tokenizer.eos_token_id]).to(device)\n",
    "    # logs('unfinished_sequences size: {}'.format(unfinished_sequences.size()), debug)\n",
    "\n",
    "    sentence_length = input_ids.size(1)\n",
    "\n",
    "    while sentence_length <= max_seq_len :\n",
    "\n",
    "        x = model.embedding(input_ids)\n",
    "        x = model.positonal_embedding(x)\n",
    "        x = model.decoder(x, enc_output)\n",
    "        next_token_logits = x[:, -1, :]\n",
    "        \n",
    "        # logs(f'next_token_logits size: {next_token_logits.size()}', debug)\n",
    "\n",
    "        next_token_logits = F.softmax(next_token_logits, dim=1)\n",
    "        next_token_indices = torch.argmax(next_token_logits, dim = 1)\n",
    "\n",
    "        # logs(f'next_token_indices post softmax size: {next_token_indices.size()}', debug)\n",
    "\n",
    "        # logs(f'next_token_indices * unfinished_sequences size: {(next_token_indices.unsqueeze(1) * unfinished_sequences).size()}', debug)\n",
    "\n",
    "        # logs(f'tokenizer.pad_token_id * (1 - unfinished_sequences) size: {(tokenizer.pad_token_id * (1 - unfinished_sequences)).size()}', debug)\n",
    "\n",
    "        next_token_indices = (\n",
    "            next_token_indices.unsqueeze(1) * unfinished_sequences + tokenizer.pad_token_id * (1 - unfinished_sequences)\n",
    "        )\n",
    "        unfinished_sequences = unfinished_sequences.mul(\n",
    "            next_token_indices.tile(\n",
    "                eos_token_id_tensor.shape[0]\n",
    "            ).ne(eos_token_id_tensor).prod(dim = 0)\n",
    "        )\n",
    "\n",
    "        if unfinished_sequences.max() == 0 :\n",
    "            break\n",
    "\n",
    "        # print(input_ids.size())\n",
    "        # print(next_token_indices.size())\n",
    "\n",
    "        input_ids = torch.cat(\n",
    "            (\n",
    "                input_ids, \n",
    "                next_token_indices\n",
    "            ), \n",
    "            dim = 1).long()\n",
    "\n",
    "        sentence_length += 1\n",
    "    \n",
    "        print(input_ids.size(1))\n",
    "\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / val loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, optimizer, criterion, device, mode = 'train') :\n",
    "\n",
    "    EPOCH_LOSS = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, rows in enumerate(data_loader[mode]) :\n",
    "\n",
    "        ## preprocess batch for training\n",
    "        en_token_ids = tokenizer.batch_encode_plus(rows['en'], add_special_tokens = False)\n",
    "        fr_token_ids = tokenizer.batch_encode_plus(rows['fr'], add_special_tokens = False)\n",
    "        encoder_inp = preprocess_batch(en_token_ids, type='input').to(device)\n",
    "        decoder_inp = preprocess_batch(fr_token_ids, type='output').to(device)\n",
    "        label = preprocess_batch(fr_token_ids, type='label').to(device)\n",
    "\n",
    "        ## forward pass through the model\n",
    "        attention_scores, output = model(encoder_inp, decoder_inp)\n",
    "\n",
    "        ## calculate loss\n",
    "        loss = criterion(output, label.reshape(-1))\n",
    "\n",
    "        ## optimize model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ## accumulate loss\n",
    "        EPOCH_LOSS += loss.item()\n",
    "\n",
    "    return EPOCH_LOSS / len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, valid_loader, device, tokenizer, max_seq_len) :\n",
    "\n",
    "    EPOCH_LOSS = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i, rows in enumerate(valid_loader) :\n",
    "\n",
    "        ## preprocess batch for training\n",
    "        en_token_ids = tokenizer.batch_encode_plus(rows['en'], add_special_tokens = False)\n",
    "        fr_token_ids = tokenizer.batch_encode_plus(rows['fr'], add_special_tokens = False)\n",
    "        encoder_inp = preprocess_batch(en_token_ids, type='input').to(device)\n",
    "        decoder_inp = preprocess_batch(fr_token_ids, type='output').to(device)\n",
    "        label = preprocess_batch(fr_token_ids, type='label').to(device)\n",
    "\n",
    "        ## encode the input\n",
    "        enc_output, attention_scores = model.encoder_pass(encoder_inp)\n",
    "\n",
    "        \n",
    "        input_tokens = generate(model, enc_output, tokenizer, max_seq_len)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"addmm_impl_cpu_\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epochs \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_epochs, end_epochs) :\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpochs \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     train_loss \u001b[39m=\u001b[39m train_model(model, data_loaders, optimizer, criterion, DEVICE, mode \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     val_loss \u001b[39m=\u001b[39m train_model(model, data_loaders, optimizer, criterion, DEVICE, mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrain Loss : \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m | Val Loss : \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(train_loss, val_loss))\n",
      "Cell \u001b[0;32mIn[45], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, optimizer, criterion, device, mode)\u001b[0m\n\u001b[1;32m     14\u001b[0m label \u001b[39m=\u001b[39m preprocess_batch(fr_token_ids, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[39m## forward pass through the model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m attention_scores, output \u001b[39m=\u001b[39m model(encoder_inp, decoder_inp)\n\u001b[1;32m     19\u001b[0m \u001b[39m## calculate loss\u001b[39;00m\n\u001b[1;32m     20\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/self-implementations/transformers_fmt/model_blocks/transformer.py:158\u001b[0m, in \u001b[0;36mTransformers.forward\u001b[0;34m(self, encoder_inp, decoder_inp)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, encoder_inp, decoder_inp) :\n\u001b[0;32m--> 158\u001b[0m     enc_output, attention_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_pass(encoder_inp)\n\u001b[1;32m    159\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_pass(enc_output, decoder_inp)\n\u001b[1;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m attention_scores, output\n",
      "File \u001b[0;32m~/Documents/self-implementations/transformers_fmt/model_blocks/transformer.py:140\u001b[0m, in \u001b[0;36mTransformers.encoder_pass\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    138\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[1;32m    139\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositonal_embedding(x)\n\u001b[0;32m--> 140\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/self-implementations/transformers_fmt/model_blocks/transformer.py:67\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x) :\n\u001b[1;32m     65\u001b[0m     \u001b[39m# logs(f'input size : {x.size()}')\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39mfor\u001b[39;00m name, layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mitems() :\n\u001b[0;32m---> 67\u001b[0m         x, attention_scores \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m     68\u001b[0m         \u001b[39m# logs(f'{name} output size : {x.size()}')\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m x, attention_scores\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/self-implementations/transformers_fmt/model_blocks/transformer_layer.py:38\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x) :\n\u001b[0;32m---> 38\u001b[0m     mha_output, mha_attention_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmha(x)\n\u001b[1;32m     39\u001b[0m     \u001b[39m# logs(f\"mha_output shape: {mha_output.shape}\")\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     norm_output1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(x, mha_output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/self-implementations/transformers_fmt/model_blocks/internal_blocks.py:143\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, q)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m# elif x.size() != q.size() :\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[39m#     raise ValueError(\"q and X must have same size\")\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39melse\u001b[39;00m :\n\u001b[1;32m    141\u001b[0m     q \u001b[39m=\u001b[39m x\n\u001b[0;32m--> 143\u001b[0m key \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw_ks(x))\n\u001b[1;32m    144\u001b[0m query \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_qs(q))\n\u001b[1;32m    145\u001b[0m value \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_vs(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"addmm_impl_cpu_\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "start_epochs = 0\n",
    "end_epochs = 10\n",
    "\n",
    "for epochs in range(start_epochs, end_epochs) :\n",
    "\n",
    "    print(f'Epochs {epochs + 1}')\n",
    "\n",
    "    train_loss = train_model(model, data_loaders, optimizer, criterion, DEVICE, mode = 'train')\n",
    "    val_loss = train_model(model, data_loaders, optimizer, criterion, DEVICE, mode = 'val')\n",
    "    \n",
    "    print('Train Loss : {:.4f} | Val Loss : {:.4f}'.format(train_loss, val_loss))\n",
    "    print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
