{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya_rustagi_farmart_co/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "## data imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "## transformer related imports\n",
    "from transformers import AutoTokenizer\n",
    "from transformers_fmt.model_blocks.transformer import Transformers\n",
    "\n",
    "## constants\n",
    "from constants import ROOT_DIR, DEVICE, LOGS_DIR\n",
    "from utils.logging import logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs dir for current run: /home/aditya_rustagi_farmart_co/fmt/self-implementations/logs/transformer_2023-08-06_09:12:01\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 20\n",
    "logs_dir = os.path.join(LOGS_DIR, f'transformer_{datetime.strftime(datetime.now(), \"%Y-%m-%d_%H:%M:%S\")}')\n",
    "print(f'Logs dir for current run: {logs_dir}')\n",
    "writer = SummaryWriter(log_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token' : '[BOS]',\n",
    "    'eos_token' : '[EOS]'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_location, chunksize=1000, n_chunks = 100) :\n",
    "\n",
    "    data = []\n",
    "\n",
    "\n",
    "    for i, items in enumerate(pd.read_csv(file_location, chunksize=chunksize)) :\n",
    "\n",
    "        data.append(items)\n",
    "        if i == n_chunks - 1 :\n",
    "            break\n",
    "\n",
    "    data = pd.concat(data)\n",
    "    data.index = range(len(data))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(file_location=os.path.join(ROOT_DIR, 'data/en_fr_100K.csv'), n_chunks = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_long_sentence(max_seq_len, data):\n",
    "    \"\"\"\n",
    "    Remove sentences that are longer than max_seq_len\n",
    "    \"\"\"\n",
    "    data['en_sentence_length'] = data['en'].apply(lambda x : len(x.split()) if type(x) == str else max_seq_len + 1)\n",
    "    data['fr_sentence_length'] = data['fr'].apply(lambda x : len(x.split()) if type(x) == str else max_seq_len + 1)\n",
    "\n",
    "    data = data.drop(\n",
    "        data[(data['en_sentence_length'] > max_seq_len) | (data['fr_sentence_length'] > max_seq_len)].index\n",
    "    )\n",
    "\n",
    "    data.index = range(len(data))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_long_sentence(max_seq_len, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "      <th>en_sentence_length</th>\n",
       "      <th>fr_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Site map</td>\n",
       "      <td>Plan du site</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feedback</td>\n",
       "      <td>Rétroaction</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Credits</td>\n",
       "      <td>Crédits</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Français</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is light ?</td>\n",
       "      <td>Qu’est-ce que la lumière?</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25016</th>\n",
       "      <td>(% change)</td>\n",
       "      <td>Tableau A-9A.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25017</th>\n",
       "      <td>YEAR</td>\n",
       "      <td>ANNÉE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25018</th>\n",
       "      <td>CONSUMER GOODS &amp; SERVICES (% change)</td>\n",
       "      <td>Biens et services de consommation ( % de chang...</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25019</th>\n",
       "      <td>FOOD &amp; NONALCOHOLIC BEVERAGES (% change)</td>\n",
       "      <td>Produits alimentaires et boissons non alcoolis...</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25020</th>\n",
       "      <td>ALCOHOLIC BEVERAGES &amp; TOBACCO (% change)</td>\n",
       "      <td>Boissons alcoolisées et tabac ( % de changement)</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25021 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en  \\\n",
       "0                                      Site map   \n",
       "1                                      Feedback   \n",
       "2                                       Credits   \n",
       "3                                      Français   \n",
       "4                               What is light ?   \n",
       "...                                         ...   \n",
       "25016                                (% change)   \n",
       "25017                                      YEAR   \n",
       "25018      CONSUMER GOODS & SERVICES (% change)   \n",
       "25019  FOOD & NONALCOHOLIC BEVERAGES (% change)   \n",
       "25020  ALCOHOLIC BEVERAGES & TOBACCO (% change)   \n",
       "\n",
       "                                                      fr  en_sentence_length  \\\n",
       "0                                           Plan du site                   2   \n",
       "1                                            Rétroaction                   1   \n",
       "2                                                Crédits                   1   \n",
       "3                                                English                   1   \n",
       "4                              Qu’est-ce que la lumière?                   4   \n",
       "...                                                  ...                 ...   \n",
       "25016                                      Tableau A-9A.                   2   \n",
       "25017                                              ANNÉE                   1   \n",
       "25018  Biens et services de consommation ( % de chang...                   6   \n",
       "25019  Produits alimentaires et boissons non alcoolis...                   6   \n",
       "25020   Boissons alcoolisées et tabac ( % de changement)                   6   \n",
       "\n",
       "       fr_sentence_length  \n",
       "0                       3  \n",
       "1                       1  \n",
       "2                       1  \n",
       "3                       1  \n",
       "4                       4  \n",
       "...                   ...  \n",
       "25016                   2  \n",
       "25017                   1  \n",
       "25018                   9  \n",
       "25019                  10  \n",
       "25020                   8  \n",
       "\n",
       "[25021 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data.index = range(len(train_data))\n",
    "val_data.index = range(len(val_data))\n",
    "test_data.index = range(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset) :\n",
    "\n",
    "    def __init__(self, data) :\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) -> any:\n",
    "        row = self.data.loc[index]\n",
    "        return {'en' : row['en'], 'fr' : row['fr']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "val_dataset = Data(val_data)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = Data(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "data_loaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader,\n",
    "    'test': test_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformers(\n",
    "    n_layer = 6,\n",
    "    n_heads = 8,\n",
    "    d_model = 512, \n",
    "    d_ff = 2048,\n",
    "    max_seq_len = 512,\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    device = DEVICE\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, type = 'input') :\n",
    "\n",
    "    ## Append the BOS and EOS token based on wether the batch is the encoder input, decoder input(output shifted left)\n",
    "    ## or the label (output shifted right)\n",
    "    if type == 'input' :\n",
    "        input_token_ids = [(torch.tensor(inp)) for inp in batch['input_ids']]\n",
    "        # input_token_ids = torch.tensor(batch['input_ids'])\n",
    "\n",
    "    elif type == 'output' :\n",
    "        input_token_ids = [\n",
    "            torch.cat(\n",
    "                (torch.tensor([tokenizer.bos_token_id]), torch.tensor(inp)),\n",
    "            ) for inp in batch['input_ids']\n",
    "        ]\n",
    "\n",
    "    elif type == 'label' :\n",
    "        input_token_ids = [\n",
    "            torch.cat(\n",
    "                (torch.tensor(inp), torch.tensor([tokenizer.eos_token_id])),\n",
    "            ) for inp in batch['input_ids']\n",
    "        ]\n",
    "\n",
    "    ## pad the token to the maxiumum sentence length\n",
    "    input_token_ids = pad_sequence(input_token_ids, batch_first=True, padding_value = tokenizer.pad_token_id)\n",
    "\n",
    "    return input_token_ids\n",
    "\n",
    "# def collate_fn(samples):\n",
    "    \n",
    "#     eng_samples = [items['en'] for items in samples]\n",
    "#     fr_samples = [items['fr'] for items in samples]\n",
    "\n",
    "#     batch = {}\n",
    "\n",
    "#     for language, sample in {'en' : eng_samples, 'fr' : fr_samples}.items() :\n",
    "\n",
    "#         sample = tokenizer.batch_encode_plus(sample)\n",
    "#         batch[language] = preprocess_batch(sample)\n",
    "\n",
    "#     # samples['fr'] = tokenizer.batch_encode_plus(samples['fr'])\n",
    "#     return batch  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / val loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "def train_model(model, data_loader, optimizer, criterion, device, epoch, mode = 'train') :\n",
    "\n",
    "    EPOCH_LOSS = 0\n",
    "\n",
    "    assert mode in ['train', 'val'], 'Mode should be either \"train\" or \"val\"'\n",
    "\n",
    "    if mode == 'train' :\n",
    "        model.train()\n",
    "    elif mode == 'val' :\n",
    "        model.eval()\n",
    "\n",
    "    for i, rows in enumerate(data_loader[mode]) :\n",
    "\n",
    "        try :\n",
    "\n",
    "            ## preprocess batch for training\n",
    "            en_token_ids = tokenizer.batch_encode_plus(rows['en'], add_special_tokens = False)\n",
    "            fr_token_ids = tokenizer.batch_encode_plus(rows['fr'], add_special_tokens = False)\n",
    "            encoder_inp = preprocess_batch(en_token_ids, type='input').to(device)\n",
    "            logs(f'encoder_inp size : {encoder_inp.size()}', debug)\n",
    "            decoder_inp = preprocess_batch(fr_token_ids, type='output').to(device)\n",
    "            logs(f'decoder_inp size : {decoder_inp.size()}', debug)\n",
    "            label = preprocess_batch(fr_token_ids, type='label').to(device)\n",
    "\n",
    "            ## encoder mask\n",
    "            enc_mask = encoder_inp != tokenizer.pad_token_id\n",
    "\n",
    "            ## forward pass through the model\n",
    "            attention_scores, output = model(encoder_inp, decoder_inp, enc_mask)\n",
    "\n",
    "            ## calculate loss\n",
    "            loss = criterion(output, label.reshape(-1))\n",
    "\n",
    "            ## optimize model\n",
    "            if mode == 'train' :\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            ## accumulate loss\n",
    "            EPOCH_LOSS += loss.item()\n",
    "\n",
    "            writer.add_scalar(f'{mode} loss', EPOCH_LOSS/(i + 1), epoch * len(data_loader[mode]) + i)\n",
    "\n",
    "        except Exception as e :\n",
    "            print(f'{epoch}_{i} | Exception : {e}')\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return EPOCH_LOSS / len(data_loader[mode])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_val_loss = 1000\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epochs = 0\n",
    "end_epochs = 100\n",
    "\n",
    "for epochs in range(start_epochs, end_epochs) :\n",
    "\n",
    "    train_loss = train_model(model, data_loaders, optimizer, criterion, DEVICE, epochs, mode = 'train')\n",
    "    val_loss = train_model(model, data_loaders, optimizer, criterion, DEVICE, epochs, mode = 'val')\n",
    "    \n",
    "    if val_loss < least_val_loss :\n",
    "        try :\n",
    "            least_val_loss = val_loss\n",
    "            torch.save(\n",
    "                model.state_dict(), \n",
    "                os.path.join(MODEL_DIR, f'transformer_ep-{epochs + 1}_val-loss-{val_loss:.4f}.pt')\n",
    "            )\n",
    "        except Exception as e :\n",
    "            print(f'{epochs} | Problem in saving model\\nException : {e}')\n",
    "\n",
    "    print(f'Epochs : {epochs + 1} | Train Loss : {train_loss:.4f} | Val Loss : {val_loss:.4f}')\n",
    "    print('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, valid_loader, device, tokenizer, max_seq_len) :\n",
    "\n",
    "    EPOCH_LOSS = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i, rows in enumerate(valid_loader) :\n",
    "\n",
    "        ## preprocess batch for training\n",
    "        en_token_ids = tokenizer.batch_encode_plus(rows['en'], add_special_tokens = False)\n",
    "        fr_token_ids = tokenizer.batch_encode_plus(rows['fr'], add_special_tokens = False)\n",
    "        encoder_inp = preprocess_batch(en_token_ids, type='input').to(device)\n",
    "        decoder_inp = preprocess_batch(fr_token_ids, type='output').to(device)\n",
    "        label = preprocess_batch(fr_token_ids, type='label').to(device)\n",
    "        \n",
    "\n",
    "        ## encode the input\n",
    "        enc_output, attention_scores = model.encoder_pass(encoder_inp)\n",
    "\n",
    "        input_tokens = generate(model, enc_output, tokenizer, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "def generate(model, enc_output, tokenizer, max_seq_len, device):\n",
    "    \n",
    "    input_ids = torch.tensor([[tokenizer.bos_token_id] for _ in range(enc_output.size(0))]).to(device)\n",
    "\n",
    "    unfinished_sequences = torch.ones(input_ids.size(0), 1).to(device)\n",
    "    eos_token_id_tensor = torch.tensor([tokenizer.eos_token_id]).to(device)\n",
    "    # logs('unfinished_sequences size: {}'.format(unfinished_sequences.size()), debug)\n",
    "\n",
    "    sentence_length = input_ids.size(1)\n",
    "\n",
    "    while sentence_length <= max_seq_len :\n",
    "\n",
    "        x = model.embedding(input_ids)\n",
    "        x = model.positonal_embedding(x)\n",
    "        x = model.decoder(x, enc_output)\n",
    "        next_token_logits = x[:, -1, :]\n",
    "        \n",
    "        logs(f'next_token_logits size: {next_token_logits.size()}', debug)\n",
    "        # print(next_token_logits)\n",
    "        next_token_logits = F.softmax(next_token_logits, dim=1)\n",
    "        next_token_indices = torch.argmax(next_token_logits, dim = 1)\n",
    "        print(next_token_indices)\n",
    "\n",
    "        logs(f'next_token_indices post softmax size: {next_token_indices.size()}', debug)\n",
    "\n",
    "        logs(f'next_token_indices * unfinished_sequences size: {(next_token_indices.unsqueeze(1) * unfinished_sequences).size()}', debug)\n",
    "\n",
    "        logs(f'tokenizer.pad_token_id * (1 - unfinished_sequences) size: {(tokenizer.pad_token_id * (1 - unfinished_sequences)).size()}', debug)\n",
    "\n",
    "        next_token_indices = (\n",
    "            next_token_indices.unsqueeze(1) * unfinished_sequences + tokenizer.pad_token_id * (1 - unfinished_sequences)\n",
    "        )\n",
    "        unfinished_sequences = unfinished_sequences.mul(\n",
    "            next_token_indices.tile(\n",
    "                eos_token_id_tensor.shape[0]\n",
    "            ).ne(eos_token_id_tensor).prod(dim = 0)\n",
    "        )\n",
    "\n",
    "        logs(unfinished_sequences, debug)\n",
    "\n",
    "        if unfinished_sequences.max() == 0 :\n",
    "            break\n",
    "\n",
    "        # print(input_ids.size())\n",
    "        # print(next_token_indices.size())\n",
    "\n",
    "        input_ids = torch.cat(\n",
    "            (\n",
    "                input_ids, \n",
    "                next_token_indices\n",
    "            ), \n",
    "            dim = 1).long()\n",
    "\n",
    "        sentence_length += 1\n",
    "    \n",
    "        print(input_ids.size(1))\n",
    "        print('-'*30)\n",
    "\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = ['my name is kong', 'this is aditya rustagi', 'i like to eat ice cream']\n",
    "test_sentence_tokens = tokenizer.batch_encode_plus(test_sentence, add_special_tokens = False)\n",
    "encoder_inp = preprocess_batch(test_sentence_tokens, type='input').to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "2\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "3\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "4\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "5\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "6\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "7\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "8\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "9\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "10\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "11\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "12\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "13\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "14\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "15\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "16\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "17\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "18\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "19\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([ 68, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "20\n",
      "------------------------------\n",
      "next_token_logits size: torch.Size([3, 512])\n",
      "tensor([508, 508, 508], device='cuda:0')\n",
      "next_token_indices post softmax size: torch.Size([3])\n",
      "next_token_indices * unfinished_sequences size: torch.Size([3, 1])\n",
      "tokenizer.pad_token_id * (1 - unfinished_sequences) size: torch.Size([3, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "21\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "enc_output, attention_scores = model.encoder_pass(encoder_inp)\n",
    "output_ids = generate(model, enc_output, tokenizer, max_seq_len, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[BOS] Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ',\n",
       " '[BOS] Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ [unused19] Џ Џ Џ',\n",
       " '[BOS] Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ Џ']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
