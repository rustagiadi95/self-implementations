{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## why not put attention heads as 3rd dimension? (BATCH_SIZE, ATTENTION_HEADS, SEQ_LEN, D_MODEL)\n",
    "## Apply padding masks to multi head attention\n",
    "## how to stack transformer encoder decoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "def logs(s):\n",
    "    if debug:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(2, 1, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5520, 0.8752, 0.9604],\n",
       "          [0.9028, 0.3016, 0.8522],\n",
       "          [0.2651, 0.7454, 0.8019],\n",
       "          [0.4348, 0.0396, 0.2529],\n",
       "          [0.8547, 0.4077, 0.3363]]],\n",
       "\n",
       "\n",
       "        [[[0.3557, 0.4578, 0.1522],\n",
       "          [0.5687, 0.0858, 0.4042],\n",
       "          [0.4029, 0.7768, 0.1005],\n",
       "          [0.8105, 0.0802, 0.7300],\n",
       "          [0.4231, 0.9843, 0.3933]]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2573, 0.3555, 0.3871],\n",
       "          [0.4002, 0.2194, 0.3804],\n",
       "          [0.2311, 0.3736, 0.3953],\n",
       "          [0.3989, 0.2686, 0.3325],\n",
       "          [0.4474, 0.2861, 0.2664]]],\n",
       "\n",
       "\n",
       "        [[[0.3421, 0.3788, 0.2791],\n",
       "          [0.4056, 0.2503, 0.3441],\n",
       "          [0.3132, 0.4553, 0.2315],\n",
       "          [0.4159, 0.2004, 0.3837],\n",
       "          [0.2686, 0.4707, 0.2607]]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(t, dim = 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module) :\n",
    "\n",
    "    def __init__(self, \n",
    "                 n_heads:int = 8,\n",
    "                 d_model:int = 512,\n",
    "                 mask:bool = False\n",
    "        ) -> None :\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_heads (int): Number of heads in the multi head attention. Defualts to 8\n",
    "            d_model (int, optional): Dimension of the input. Defaults to 512.\n",
    "            mask (bool, optional): Whether to apply masking. Defaults to False\n",
    "        \"\"\"\n",
    "\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.mask = mask\n",
    "        self.d_k = int(d_model/n_heads)\n",
    "\n",
    "    def forward(self,\n",
    "                key : torch.Tensor,\n",
    "                query : torch.Tensor,\n",
    "                value : torch.Tensor\n",
    "        ) -> torch.Tensor :\n",
    "\n",
    "        \"\"\"\n",
    "        Calculate scaler dot product of key, query and values as described in https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "        Args:\n",
    "            key (torch.Tensor): Key tensor. Shape = (n_heads, batch_size, seq_len, d_model/n_heads)\n",
    "            query (torch.Tensor): Query tensor. Shape = (n_heads, batch_size, seq_len, d_model/n_heads)\n",
    "            value (torch.Tensor): Value tensor. Shape = (n_heads, batch_size, seq_len, d_model/n_heads)\n",
    "\n",
    "        Returns:\n",
    "            value_with_attention: Value with attention applied. Shape = (n_heads, batch_size, seq_len, d_model/n_heads)\n",
    "        \"\"\"\n",
    "\n",
    "        assert key.size() == query.size() == value.size(), \"Key, query and value must have same shape\"\n",
    "\n",
    "        batch_size, seq_len = key.size(1), key.size(2)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(2, 3))/torch.sqrt(torch.tensor(self.d_k))\n",
    "        attention_scores = torch.softmax(attention_scores, dim = 3)\n",
    "        \n",
    "        if self.mask :\n",
    "            mask = torch.ones(self.n_heads, batch_size, seq_len, seq_len)\n",
    "            mask = torch.tril(mask)\n",
    "            attention_scores = torch.matmul(attention_scores, mask)\n",
    "            \n",
    "        value_with_attention = torch.matmul(attention_scores, value)\n",
    "\n",
    "        return value_with_attention, attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) :\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_head: int = 8, \n",
    "                 d_model: int = 512, \n",
    "                 dropout: float = 0.1, \n",
    "                 mask: bool = False,\n",
    "                 self_attention:bool = True\n",
    "        ) :\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_head (int): Number of heads. Defaults to 8.\n",
    "            d_model (int): Dimension of input. Defaults to 512.\n",
    "            dropout (float): Dropout rate. Defaults to 0.1.\n",
    "            mask (bool): Whether to mask the attention. Defaults to False.\n",
    "            self_attention (bool): Whether to use self attention. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.self_attention = self_attention\n",
    "\n",
    "        self.d_k = self.d_v = d_model // n_head\n",
    "        self.w_qs = nn.Linear(d_model, n_head * self.d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * self.d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * self.d_v)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(n_head, d_model, mask)\n",
    "\n",
    "        self.mha_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        nn.init.normal_(self.w_qs.weight, mean = 0, std = np.sqrt(2.0 / (d_model + self.d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean = 0, std = np.sqrt(2.0 / (d_model + self.d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean = 0, std = np.sqrt(2.0 / (d_model + self.d_v)))\n",
    "\n",
    "    def forward(self, x, q = None) :\n",
    "\n",
    "        \"\"\"\n",
    "        Implementation of multi head attention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Padded input with the shaep batch_len, seq_len, d_model\n",
    "            q (torch.Tensor): Query with the shape batch_size, seq_len, d_model. Defaults to None.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Values with multiheadattention applied. Shape = (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If mode is cross attention and query passed in forward is None.\n",
    "            ValueError: If mode is cross attention and shape of query is not same as input coming from encoder.\n",
    "        \n",
    "        References:\n",
    "            https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/MultiHeadAttention.py\n",
    "            https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Transformer.py\n",
    "            https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/PositionalEncoding.py\n",
    "        \"\"\"\n",
    "         \n",
    "        if not self.self_attention:\n",
    "            if q is None :\n",
    "                raise ValueError(\"q is required for cross attention\")\n",
    "            elif x.size() != q.size() :\n",
    "                raise ValueError(\"q and X must have same size\")\n",
    "        else :\n",
    "            q = x\n",
    "\n",
    "        key = F.gelu(self.w_ks(x))\n",
    "        query = F.gelu(self.w_qs(q))\n",
    "        value = F.gelu(self.w_vs(x))\n",
    "\n",
    "        ## keeping n_heads as major dimension\n",
    "        key = key.view(-1, key.size(0), key.size(1), self.d_k)\n",
    "        query = query.view(-1, query.size(0), query.size(1), self.d_k)\n",
    "        value = value.view(-1, value.size(0), value.size(1), self.d_v)\n",
    "\n",
    "        value, attention = self.attention(key, query, value)\n",
    "        logs(f'values with attention size : {value.size()}')\n",
    "\n",
    "        value = value.view(value.size(1), value.size(2), -1)\n",
    "        logs(f'values with attention after reshape size : {value.size()}')\n",
    "\n",
    "        value = self.dropout(value)\n",
    "\n",
    "        value = F.gelu(self.mha_linear(value))\n",
    "\n",
    "        return value, attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add and layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayerNormalization(nn.Module) :\n",
    "\n",
    "    def __init__(self, sequence_len, d_model) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm([sequence_len, d_model])\n",
    "\n",
    "    def forward(self, x, mha_output) :\n",
    "\n",
    "        return self.layer_norm(x + mha_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point Wise Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedforward(nn.Module) :\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_ff: int = 2048, \n",
    "                 d_model: int = 512\n",
    "    ) -> None :\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_ff (int): Intermediate size of the feedforward layer.\n",
    "            d_model (int):  Size of the embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "        super(PointWiseFeedforward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x) :\n",
    "\n",
    "        linear1_output = self.linear1(x)\n",
    "        linear2_output = self.linear2(F.gelu(linear1_output))\n",
    "\n",
    "        return linear2_output\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Single Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module) :\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_heads: int = 8,\n",
    "                 max_seq_len: int = 64,\n",
    "                 d_model: int = 512,\n",
    "                 d_ff: int = 2048\n",
    "        ) -> None :\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(n_heads, d_model, )\n",
    "        self.layer_norm = AddLayerNormalization(max_seq_len, d_model)\n",
    "        self.pff = PointWiseFeedforward(d_ff, d_model)\n",
    "        self.layer_norm2 = AddLayerNormalization(max_seq_len, d_model)\n",
    "\n",
    "    def forward(self, x) :\n",
    "\n",
    "        mha_output, mha_attention_scores = self.mha(x)\n",
    "        norm_output1 = self.layer_norm(x, mha_output)\n",
    "\n",
    "        pff_output = self.pff(norm_output1)\n",
    "        norm_output2 = self.layer_norm2(norm_output1, pff_output)\n",
    "\n",
    "        return norm_output2, mha_attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_layer: int = 6,\n",
    "                 n_heads: int = 8,\n",
    "                 max_seq_len: int = 64,\n",
    "                 d_model: int = 512,\n",
    "                 d_ff: int = 2048\n",
    "    ) :\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.ModuleDict({\n",
    "            'encoder_layer_{}'.format(i) : \n",
    "            (\n",
    "                EncoderLayer(\n",
    "                    n_heads,\n",
    "                    max_seq_len,\n",
    "                    d_model,\n",
    "                    d_ff\n",
    "                )\n",
    "            ) for i in range(n_layer)\n",
    "            })\n",
    "\n",
    "    def forward(self, x) :\n",
    "        logs(f'input size : {x.size()}')\n",
    "        for name, layer in self.encoder.items() :\n",
    "            x, attention_scores = layer(x)\n",
    "            logs(f'{name} output size : {x.size()}')\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
