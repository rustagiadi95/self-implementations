{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## why not put attention heads as 3rd dimension? (BATCH_SIZE, ATTENTION_HEADS, SEQ_LEN, D_MODEL)\n",
    "## Apply padding masks to multi head attention\n",
    "## how to stack transformer encoder decoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityarustagi/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers_si.model_blocks.transformer import Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.logging import logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/Users/adityarustagi/Documents/self-implementations/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module) :\n",
    "\n",
    "    def __init__(self, \n",
    "                 n_heads:int = 8,\n",
    "                 d_model:int = 512,\n",
    "                 mask:bool = False\n",
    "        ) -> None :\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_heads (int): Number of heads in the multi head attention. Defualts to 8\n",
    "            d_model (int, optional): Dimension of the input. Defaults to 512.\n",
    "            mask (bool, optional): Whether to apply masking. Defaults to False\n",
    "        \"\"\"\n",
    "\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.mask = mask\n",
    "        self.d_k = int(d_model/n_heads)\n",
    "\n",
    "    def forward(self,\n",
    "                key : torch.Tensor,\n",
    "                query : torch.Tensor,\n",
    "                value : torch.Tensor\n",
    "        ) -> torch.Tensor :\n",
    "\n",
    "        \"\"\"\n",
    "        Calculate scaler dot product of key, query and values as described in https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "        Args:\n",
    "            key (torch.Tensor): Key tensor. Shape = (n_heads, batch_size, seq_len, d_model/n_heads)\n",
    "            query (torch.Tensor): Query tensor. Shape = (n_heads, batch_size, seq_len, d_model/n_heads)\n",
    "            value (torch.Tensor): Value tensor. Shape = (n_heads, batch_size, seq_len, d_model/n_heads)\n",
    "\n",
    "        Returns:\n",
    "            value_with_attention: Value with attention applied. Shape = (n_heads, batch_size, seq_len, d_model/n_heads)\n",
    "        \"\"\"\n",
    "\n",
    "        # assert key.size() == query.size() == value.size(), \"Key, query and value must have same shape\"\n",
    "\n",
    "        batch_size, seq_len = key.size(1), key.size(2)\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(2, 3))/torch.sqrt(torch.tensor(self.d_k))\n",
    "        attention_scores = torch.softmax(attention_scores, dim = 3)\n",
    "        \n",
    "        if self.mask :\n",
    "            mask = torch.ones(self.n_heads, batch_size, seq_len, seq_len)\n",
    "            mask = torch.tril(mask)\n",
    "            attention_scores = torch.matmul(attention_scores, mask)\n",
    "            \n",
    "        value_with_attention = torch.matmul(attention_scores, value)\n",
    "\n",
    "        return value_with_attention, attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module) :\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_head: int = 8, \n",
    "                 d_model: int = 512, \n",
    "                 dropout: float = 0.1, \n",
    "                 mask: bool = False,\n",
    "                 self_attention:bool = True\n",
    "        ) :\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_head (int): Number of heads. Defaults to 8.\n",
    "            d_model (int): Dimension of input. Defaults to 512.\n",
    "            dropout (float): Dropout rate. Defaults to 0.1.\n",
    "            mask (bool): Whether to mask the attention. Defaults to False.\n",
    "            self_attention (bool): Whether to use self attention. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.self_attention = self_attention\n",
    "\n",
    "        self.d_k = self.d_v = d_model // n_head\n",
    "        self.w_qs = nn.Linear(d_model, n_head * self.d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * self.d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * self.d_v)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(n_head, d_model, mask)\n",
    "\n",
    "        self.mha_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        nn.init.normal_(self.w_qs.weight, mean = 0, std = np.sqrt(2.0 / (d_model + self.d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean = 0, std = np.sqrt(2.0 / (d_model + self.d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean = 0, std = np.sqrt(2.0 / (d_model + self.d_v)))\n",
    "\n",
    "    def forward(self, x, q = None) :\n",
    "\n",
    "        \"\"\"\n",
    "        Implementation of multi head attention layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Padded input with the shaep batch_len, seq_len, d_model\n",
    "            q (torch.Tensor): Query with the shape batch_size, seq_len, d_model. Defaults to None.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Values with multiheadattention applied. Shape = (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If mode is cross attention and query passed in forward is None.\n",
    "            ValueError: If mode is cross attention and shape of query is not same as input coming from encoder.\n",
    "        \n",
    "        References:\n",
    "            https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/MultiHeadAttention.py\n",
    "            https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Transformer.py\n",
    "            https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/PositionalEncoding.py\n",
    "        \"\"\"\n",
    "         \n",
    "        if not self.self_attention:\n",
    "            if q is None :\n",
    "                raise ValueError(\"q is required for cross attention\")\n",
    "            # elif x.size() != q.size() :\n",
    "            #     raise ValueError(\"q and X must have same size\")\n",
    "        else :\n",
    "            q = x\n",
    "\n",
    "        key = F.gelu(self.w_ks(x))\n",
    "        query = F.gelu(self.w_qs(q))\n",
    "        value = F.gelu(self.w_vs(x))\n",
    "\n",
    "        ## keeping n_heads as major dimension\n",
    "        key = key.view(-1, key.size(0), key.size(1), self.d_k)\n",
    "        query = query.view(-1, query.size(0), query.size(1), self.d_k)\n",
    "        value = value.view(-1, value.size(0), value.size(1), self.d_v)\n",
    "\n",
    "        value, attention = self.attention(key, query, value)\n",
    "\n",
    "        value = value.view(value.size(1), value.size(2), -1)\n",
    "\n",
    "        value = self.dropout(value)\n",
    "\n",
    "        value = F.gelu(self.mha_linear(value))\n",
    "\n",
    "        return value, attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add and layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayerNormalization(nn.Module) :\n",
    "\n",
    "    def __init__(self, d_model) :\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm([d_model])\n",
    "\n",
    "    def forward(self, x, mha_output) :\n",
    "\n",
    "        return self.layer_norm(x + mha_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point Wise Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedforward(nn.Module) :\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_ff: int = 2048, \n",
    "                 d_model: int = 512\n",
    "    ) -> None :\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_ff (int): Intermediate size of the feedforward layer.\n",
    "            d_model (int):  Size of the embeddings.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(PointWiseFeedforward, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x) :\n",
    "\n",
    "        linear1_output = self.linear1(x)\n",
    "        linear2_output = self.linear2(F.gelu(linear1_output))\n",
    "\n",
    "        return linear2_output\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Single Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module) :\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_heads: int = 8,\n",
    "                 d_model: int = 512,\n",
    "                 d_ff: int = 2048\n",
    "        ) -> None :\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(n_heads, d_model, )\n",
    "        self.layer_norm = AddLayerNormalization(d_model)\n",
    "        self.pff = PointWiseFeedforward(d_ff, d_model)\n",
    "        self.layer_norm2 = AddLayerNormalization(d_model)\n",
    "\n",
    "    def forward(self, x) :\n",
    "\n",
    "        mha_output, mha_attention_scores = self.mha(x)\n",
    "        # logs(f\"mha_output shape: {mha_output.shape}\")\n",
    "        norm_output1 = self.layer_norm(x, mha_output)\n",
    "        # logs(f\"norm_output1 shape: {norm_output1.shape}\")\n",
    "\n",
    "        pff_output = self.pff(norm_output1)\n",
    "        # logs(f\"pff_output shape: {pff_output.shape}\")\n",
    "        norm_output2 = self.layer_norm2(norm_output1, pff_output)\n",
    "        # logs(f\"norm_output2 shape: {norm_output2.shape}\")\n",
    "\n",
    "        return norm_output2, mha_attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module) :\n",
    "\n",
    "    def __init__(self, \n",
    "                 n_heads,\n",
    "                 d_model,\n",
    "                 d_ff, \n",
    "    ) -> None :\n",
    "\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(n_head=n_heads, d_model=d_model, mask = True)\n",
    "        self.cross_mha = MultiHeadAttention(n_head=n_heads, d_model=d_model, self_attention=False)\n",
    "        self.layer_norm1 = AddLayerNormalization(d_model)\n",
    "        self.layer_norm2 = AddLayerNormalization(d_model)\n",
    "        self.layer_norm3 = AddLayerNormalization(d_model)\n",
    "        self.pff = PointWiseFeedforward(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x, enc_out) :\n",
    "        ## passing encoder output to all decoder layers : to be discussed with Deepak\n",
    "        decoder_query, _ = self.mha(x)\n",
    "        norm_decoder_query = self.layer_norm1(x, decoder_query)\n",
    "\n",
    "        x, _ = self.cross_mha(enc_out, norm_decoder_query)\n",
    "        norm_cross_x = self.layer_norm2(norm_decoder_query, x)\n",
    "\n",
    "        x = self.pff(norm_cross_x)\n",
    "        norm_decoder_output = self.layer_norm3(norm_cross_x, x)\n",
    "\n",
    "        return norm_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module) :\n",
    "\n",
    "    def __init__(self,\n",
    "        max_seq_len: int = 128, \n",
    "        d_model: int = 512,\n",
    "    ) :\n",
    "\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "\n",
    "        self.embedding = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        for i in range(max_seq_len) :\n",
    "            self.embedding[i, 0::2] = torch.sin((i/1000**(2*torch.arange(512)[::2]/512)))\n",
    "            self.embedding[i, 1::2] = torch.cos((i/1000**(2*torch.arange(512)[1::2]/512)))\n",
    "\n",
    "    def forward(self, x) :\n",
    "\n",
    "        embedding = torch.repeat_interleave(self.embedding.unsqueeze(0), x.size(0), 0)\n",
    "\n",
    "        return x + embedding[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_layer: int = 6,\n",
    "                 n_heads: int = 8,\n",
    "                 d_model: int = 512,\n",
    "                 d_ff: int = 2048\n",
    "    ) :\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.ModuleDict({\n",
    "            f'encoder_layer_{i}' : \n",
    "            (\n",
    "                EncoderLayer(\n",
    "                    n_heads,\n",
    "                    d_model,\n",
    "                    d_ff\n",
    "                )\n",
    "            ) for i in range(n_layer)\n",
    "            })\n",
    "\n",
    "    def forward(self, x) :\n",
    "        # logs(f'input size : {x.size()}')\n",
    "        for name, layer in self.encoder.items() :\n",
    "            x, attention_scores = layer(x)\n",
    "            # logs(f'{name} output size : {x.size()}')\n",
    "        return x, attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_layer: int = 6,\n",
    "                 n_heads: int = 8,\n",
    "                 d_model: int = 512,\n",
    "                 d_ff: int = 2048\n",
    "    ) -> None :\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decoder = nn.ModuleDict({\n",
    "            f'decoder_layer_{i}' :\n",
    "            (\n",
    "                DecoderLayer(\n",
    "                    n_heads,\n",
    "                    d_model,\n",
    "                    d_ff\n",
    "                )\n",
    "            ) for i in range(n_layer)\n",
    "        })\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_out) :\n",
    "\n",
    "        for name, layer in self.decoder.items() :\n",
    "            x = layer(x, enc_out)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformers(nn.Module) :\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_layer,\n",
    "                 n_heads,\n",
    "                 d_model,\n",
    "                 d_ff,\n",
    "                 max_seq_len,\n",
    "                 vocab_size,\n",
    "        ) -> None :\n",
    "\n",
    "        super(Transformers, self).__init__()\n",
    "\n",
    "\n",
    "        self.encoder = Encoder(n_layer, n_heads, d_model, d_ff)\n",
    "        self.decoder = Decoder(n_layer, n_heads, d_model, d_ff)\n",
    "        self.positonal_embedding = PositionEmbedding(max_seq_len, d_model)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size + 3, d_model)\n",
    "        self.logit_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    \n",
    "    def preprocess(self, inputs_token_ids) :\n",
    "        \n",
    "        input_token_ids = [\n",
    "            torch.cat(\n",
    "                (torch.tensor([self.bos_token_id]), inp),\n",
    "            ) for inp in inputs_token_ids\n",
    "        ]\n",
    "\n",
    "        input_token_ids = pad_sequence(input_token_ids, batch_first=True, padding_value = self.pad_token_id)\n",
    "\n",
    "        input_token_ids = torch.cat(\n",
    "            (input_token_ids, torch.tensor([[self.eos_token_id] for _ in range(input_token_ids.size(0))])),\n",
    "            dim = 1\n",
    "        )\n",
    "\n",
    "        return input_token_ids\n",
    "\n",
    "    def encode(self, x) :\n",
    "        logs('encoding_now ------------------------------------')\n",
    "        x = self.embedding(x)\n",
    "        x = self.positonal_embedding(x)\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def generate(self, enc_output, input_ids) :\n",
    "\n",
    "        logs('generating_now ------------------------------------')\n",
    "\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.positonal_embedding(x)\n",
    "        x = self.decoder(x, enc_output)\n",
    "        next_token_logits = F.relu(self.logit_layer(x))\n",
    "        return F.softmax(next_token_logits, dim=2)\n",
    "        # sentence_length = input_ids.size(1)\n",
    "\n",
    "        # while sentence_length < self.max_seq_len :\n",
    "\n",
    "        #     x = self.embedding(input_ids)\n",
    "        #     x = self.positonal_embedding(x)\n",
    "        #     x = self.decoder(x, enc_output)\n",
    "        #     next_token_logits = x[:, -1, :]\n",
    "        #     \n",
    "            \n",
    "        #     # logs(f'next_token_logits size: {next_token_logits.size()}')\n",
    "        #     next_token_logits = F.softmax(next_token_logits, dim=1)\n",
    "        #     next_token_indices = torch.argmax(next_token_logits, dim = 1)\n",
    "        #     # logs(f'next_token_logits size: {next_token_indices.size()}')\n",
    "\n",
    "        #     input_ids = torch.cat(\n",
    "        #         (\n",
    "        #             input_ids, \n",
    "        #             next_token_indices.unsqueeze(1)\n",
    "        #         ), \n",
    "        #         dim = 1)\n",
    "\n",
    "        #     sentence_length += 1\n",
    "\n",
    "    def forward(self, encoder_inp, decoder_inp) :\n",
    "\n",
    "        # logs(f'input : {x}')\n",
    "        # logs(f'input_shape : {x.size()}')\n",
    "        enc_output, attention_scores = self.encode(encoder_inp)\n",
    "        output = self.generate(enc_output, decoder_inp)\n",
    "\n",
    "        return attention_scores, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token' : '[BOS]',\n",
    "    'eos_token' : '[EOS]'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_location, chunksize=1000, n_chunks = 100) :\n",
    "\n",
    "    data = []\n",
    "\n",
    "\n",
    "    for i, items in enumerate(pd.read_csv(file_location, chunksize=chunksize)) :\n",
    "\n",
    "        data.append(items)\n",
    "        if i == n_chunks :\n",
    "            break\n",
    "\n",
    "    data = pd.concat(data)\n",
    "    data.index = range(len(data))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(file_location=os.path.join(ROOT_DIR, 'data/en-fr.csv'), n_chunks = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset) :\n",
    "\n",
    "    def __init__(self, data) :\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) -> any:\n",
    "        row = self.data.loc[index]\n",
    "        return {'en' : row['en'], 'fr' : row['fr']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, type = 'input') :\n",
    "\n",
    "    ## Append the BOS and EOS token based on wether the batch is the encoder input, decoder input(output shifted left)\n",
    "    ## or the label (output shifted right)\n",
    "    if type == 'input' :\n",
    "        input_token_ids = [\n",
    "            torch.cat(\n",
    "                (torch.tensor([tokenizer.bos_token_id]), torch.tensor(inp), torch.tensor([tokenizer.eos_token_id])),\n",
    "            ) for inp in batch['input_ids']\n",
    "        ]\n",
    "\n",
    "    elif type == 'output' :\n",
    "        input_token_ids = [\n",
    "            torch.cat(\n",
    "                (torch.tensor([tokenizer.bos_token_id]), torch.tensor(inp)),\n",
    "            ) for inp in batch['input_ids']\n",
    "        ]\n",
    "\n",
    "    elif type == 'label' :\n",
    "        input_token_ids = [\n",
    "            torch.cat(\n",
    "                (torch.tensor(inp), torch.tensor([tokenizer.eos_token_id])),\n",
    "            ) for inp in batch['input_ids']\n",
    "        ]\n",
    "\n",
    "    ## pad the token to the maxiumum sentence length\n",
    "    input_token_ids = pad_sequence(input_token_ids, batch_first=True, padding_value = tokenizer.pad_token_id)\n",
    "\n",
    "    return input_token_ids\n",
    "\n",
    "# def collate_fn(samples):\n",
    "    \n",
    "#     eng_samples = [items['en'] for items in samples]\n",
    "#     fr_samples = [items['fr'] for items in samples]\n",
    "\n",
    "#     batch = {}\n",
    "\n",
    "#     for language, sample in {'en' : eng_samples, 'fr' : fr_samples}.items() :\n",
    "\n",
    "#         sample = tokenizer.batch_encode_plus(sample)\n",
    "#         batch[language] = preprocess_batch(sample)\n",
    "\n",
    "#     # samples['fr'] = tokenizer.batch_encode_plus(samples['fr'])\n",
    "#     return batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = Transformers(\n",
    "    n_layer=6,\n",
    "    n_heads=8,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    max_seq_len=128,\n",
    "    vocab_size = tokenizer.vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 58]) torch.Size([16, 57]) torch.Size([16, 57])\n",
      "encoding_now ------------------------------------\n",
      "generating_now ------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(encoder_inp\u001b[39m.\u001b[39msize(), decoder_inp\u001b[39m.\u001b[39msize(), label\u001b[39m.\u001b[39msize())\n\u001b[1;32m     10\u001b[0m attention_scores, output \u001b[39m=\u001b[39m transformers(encoder_inp, decoder_inp)\n\u001b[0;32m---> 11\u001b[0m criterion(label\u001b[39m.\u001b[39;49mlong(), output\u001b[39m.\u001b[39;49mlong())\n\u001b[1;32m     12\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)#, collate_fn=collate_fn\n",
    "\n",
    "for idx, rows in enumerate(dataloader) :\n",
    "    transformers.zero_grad()\n",
    "    input_token_ids = tokenizer.batch_encode_plus(rows['en'], add_special_tokens=False)\n",
    "    encoder_inp = preprocess_batch(input_token_ids, type='input')\n",
    "    decoder_inp = preprocess_batch(input_token_ids, type='output')\n",
    "    label = preprocess_batch(input_token_ids, type='label')\n",
    "    print(encoder_inp.size(), decoder_inp.size(), label.size())\n",
    "    attention_scores, output = transformers(encoder_inp, decoder_inp)\n",
    "    criterion(label.long(), output.long())\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 57, 119547]) torch.Size([16, 57])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39msize(), label\u001b[39m.\u001b[39msize())\n\u001b[0;32m----> 2\u001b[0m criterion(label, output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "print(output.size(), label.size())\n",
    "criterion(label, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 57]) torch.Size([16, 57, 512])\n"
     ]
    }
   ],
   "source": [
    "print(label.size(), output.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Transformers' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformers\u001b[39m.\u001b[39;49mdtype\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Transformers' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "transformers = Transformers(\n",
    "    n_layer=6,\n",
    "    n_heads=8,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    max_seq_len=64,\n",
    "    batch_size=3,\n",
    "    vocab_size = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
